{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load library\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID', 'non-COVID']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"./sars-covid-data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory path:  ./sars-covid-data\n",
      "Folder name:  ['COVID', 'non-COVID']\n",
      "Directory path:  ./sars-covid-data\\COVID\n",
      "Folder name:  []\n",
      "Directory path:  ./sars-covid-data\\non-COVID\n",
      "Folder name:  []\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "for (dirpath, dirnames, filenames) in walk(\"./sars-covid-data\"):\n",
    "    print(\"Directory path: \", dirpath)\n",
    "    print(\"Folder name: \", dirnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.Grayscale(),#make greyscale one channel\n",
    "    transforms.ToTensor(), #0-1, numpy to tensor\n",
    "    transforms.Normalize([0.5],\n",
    "                        [0.5]) # 0-1 to -1,1 and normalized\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "train_path ='./sars-covid-data'\n",
    "dataset = datasets.ImageFolder(train_path, transform=transformer)\n",
    "# either make here one channel or remove two channels later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5216, 0.5451, 0.5294,  ..., 0.5451, 0.5451, 0.5216],\n",
       "         [0.5451, 0.5216, 0.5216,  ..., 0.5216, 0.5373, 0.5216],\n",
       "         [0.5529, 0.5294, 0.5373,  ..., 0.5294, 0.5294, 0.5137],\n",
       "         ...,\n",
       "         [0.5608, 0.5608, 0.5608,  ..., 0.7490, 0.7490, 0.7412],\n",
       "         [0.9216, 0.9137, 0.9137,  ..., 0.9529, 0.9529, 0.9529],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_subset = torch.utils.data.Subset(dataset, np.random.choice(len(dataset), 2, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(dataset, batch_size =128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_subset, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4824, -0.4824, -0.4745,  ..., -0.4902, -0.4745, -0.4667],\n",
       "         [-0.4980, -0.4824, -0.4745,  ..., -0.4196, -0.4745, -0.4745],\n",
       "         [-0.4902, -0.4667, -0.4745,  ..., -0.4196, -0.4275, -0.4824],\n",
       "         ...,\n",
       "         [-0.2471, -0.3255, -0.0275,  ...,  0.6157,  0.6549,  0.7882],\n",
       "         [ 0.4353, -0.0039, -0.3020,  ...,  0.6235,  0.7725,  0.8118],\n",
       "         [ 0.7255,  0.6471,  0.1294,  ...,  0.7882,  0.8196,  0.0902]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(labels)\n",
    "#images.size()\n",
    "dataset_subset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_shape\"], out_features=128\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_hidden_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=kwargs[\"input_shape\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.relu(code)\n",
    "        activation = self.decoder_hidden_layer(code)\n",
    "        activation = torch.relu(activation)\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.relu(activation)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch.nn.Linear layer creates a linear function (Î¸x + b), with its parameters initialized (by default). This means we will call an activation/non-linearity for such layers.\n",
    "\n",
    "The in_features parameter dictates the feature size of the input tensor to a particular layer, e.g. in self.encoder_hidden_layer, it accepts an input tensor with the size of [N, input_shape] where N is the number of examples, and input_shape is the number of features in one example.\n",
    "\n",
    "The out_features parameter dictates the feature size of the output tensor of a particular layer. Hence, in the self.decoder_output_layer, the feature size is kwargs[\"input_shape\"], denoting that it reconstructs the original data input.\n",
    "\n",
    "The forward() function defines the forward pass for a model. This is the function invoked when we pass input tensors to an instantiated object of a torch.nn.Module class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE(input_shape=22500).to(device)\n",
    "# 150*150\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_loader:\n",
    "        # reshape mini-batch data to [N, 22500] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.view(-1, 22500).to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    \n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, 16, kernel_size = 3, stride=1, padding = 1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(16, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(0.1),           \n",
    "            nn.MaxPool2d(2,2), #(32*32*32)\n",
    "\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32768,128),#in features= , out features = 128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNnet().to(device)\n",
    "\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "#num_epochs = \n",
    "#for epoch in range(num_epochs):\n",
    "\n",
    "for i, (images,labels) in enumerate (train_loader):\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs,labels.float())\n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "    #backword proprgation, adam optim\n",
    "    #empty of grad \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #update grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    #accuracy\n",
    "    total = labels.size(0)\n",
    "    _,predicted = torch.max(outputs.data,1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    acc_list.append(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
